{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN을 만들어봅시다 갠갠갠 갠 널 사랑하지 않아"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class 만드는데 궁금리스트\n",
    "#### Batch Normalization Momentum\n",
    "#### initial Dense Layer\n",
    "#### upsample\n",
    "#### z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Dropout, Flatten, UpSampling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self\n",
    "        , input_dim #input 값의 dimension 일단 여기 animal GAN에서는 28 * 28 * 흑백(1) 짜리 사용\n",
    "        , discriminator_conv_filters # conv_filter 64 * 64 * 128 * 128 - filter가 총 네층인데 64,64,128,128로 하겠다는 뜻\n",
    "        , discriminator_conv_kernel_size # kernel_size 5 * 5* 5* 5\n",
    "        , discriminator_conv_strides # stride 2* 2* 2* 1\n",
    "                 \n",
    "        , discriminator_batch_norm_momentum # Batch Normalization 하냐 안하냐 인가 이게\n",
    "        , discriminator_activation # Activation function\n",
    "        , discriminator_dropout_rate # dropout_rate 층 깔면 얼마나 ?? \n",
    "        , discriminator_learning_rate # lr\n",
    "                 \n",
    "        , generator_initial_dense_layer_size # 왜 intital dense_layer_size지\n",
    "        , generator_upsample # upsamle의 사이즈를 말하는 건가요 2*2*1*1 이긴하네요\n",
    "        , generator_conv_filters #conv의 필터\n",
    "        , generator_conv_kernel_size #kernel size\n",
    "        , generator_conv_strides # stride\n",
    "        , generator_batch_norm_momentum # batch_norm_momentum 모멘텀이네요 하냐 안하냐가 아니라 0.9로 되어있는데 왜지\n",
    "        , generator_activation # Activation function\n",
    "        , generator_dropout_rate \n",
    "        , generator_learning_rate\n",
    "        , optimiser\n",
    "        , z_dim # 다시 모르겠는 z_dim\n",
    "        ):\n",
    "\n",
    "        self.name = 'gan' # 이름은 Gan 이라고 지을께요\n",
    "\n",
    "        self.input_dim = input_dim \n",
    "        self.discriminator_conv_filters = discriminator_conv_filters\n",
    "        self.discriminator_conv_kernel_size = discriminator_conv_kernel_size\n",
    "        self.discriminator_conv_strides = discriminator_conv_strides\n",
    "        self.discriminator_batch_norm_momentum = discriminator_batch_norm_momentum\n",
    "        self.discriminator_activation = discriminator_activation\n",
    "        self.discriminator_dropout_rate = discriminator_dropout_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "\n",
    "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
    "        self.generator_upsample = generator_upsample\n",
    "        self.generator_conv_filters = generator_conv_filters\n",
    "        self.generator_conv_kernel_size = generator_conv_kernel_size\n",
    "        self.generator_conv_strides = generator_conv_strides\n",
    "        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n",
    "        self.generator_activation = generator_activation\n",
    "        self.generator_dropout_rate = generator_dropout_rate\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "        \n",
    "        self.optimiser = optimiser\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        \n",
    "        self.n_layers_discriminator = len(discriminator_conv_filters) # discriminator의 layer 몇 개 깔지 위의 64,64,128,128 총 네 개\n",
    "        self.n_layers_generator = len(generator_conv_filters) # generator 또한 같다\n",
    "\n",
    "        self.weight_init = RandomNormal(mean=0., stddev=0.02) # 초기 가중치를 0 짜리 표준편차 0.02로 만들어봅시다\n",
    "\n",
    "        self.d_losses = []  # 로스 리스트\n",
    "        self.g_losses = [] # generator 로스 리스트\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "        self._build_discriminator() # 시작과 동시에 discriminator, generator, 만들고\n",
    "        self._build_generator()\n",
    "\n",
    "        self._build_adversarial() # adversarial --> 이거 D, G 연결한거겠지??\n",
    "        \n",
    "        # Activation Function 치면 그거 따라서 나와야하는 거 같은데 keras의 Activation Layer가 어케 작동하는거지?? ㅇㄹ\n",
    "        # keras.layer.Activation은 앞선 층의 Activation argument를 입력해주는 듯하다. 근데 왜 굳이 Leaky_Relu만 저렇게 칠꺼라고 생각하는 걸까 뿌우 \n",
    "        # 걍 알아서 하라고 하자\n",
    "        \n",
    "   # def get_activation(self, activation): \n",
    "    #    if activation == 'leaky_relu':\n",
    "     #       layer = LeakyReLU(alpha = 0.2)\n",
    "      #  else:\n",
    "       #     layer = Activation(activation)\n",
    "       # return layer\n",
    "    \n",
    "    \n",
    "    def _build_discriminator(self):\n",
    "        discriminator_input = Input(shape=self.input_dim, name= \"discriminator_input\") # Input 값 self 에서 받아서 쓰고\n",
    "        x = discriminator_input\n",
    "        \n",
    "        \n",
    "        for i in range(self.n_layers_discriminator): # discriminator 레이어층 수만큼\n",
    "            \n",
    "            x = Conv2D(\n",
    "                filters = self.discriminator_conv_filters[i]\n",
    "                , kernel_size = self.discriminator_conv_kernel_size[i]\n",
    "                , strides = self.discriminator_conv_strides[i]\n",
    "                , padding = 'same'\n",
    "                , name = 'discriminator_conv_'+str(i)\n",
    "            )(x)\n",
    "            if self.discriminator_batch_norm_momentum and i > 0: # 0번째는 안만들어?? Conv2D 밑에,\n",
    "                # 보통 Batch Normalization 은 fully connected 나 Conv2D 다음, Activation 전에 들어온다 \n",
    "                \n",
    "                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n",
    "            \n",
    "            x = Activation(self.discriminator_activation)(x)\n",
    "            \n",
    "            if self.discriminator_dropout_rate :\n",
    "                x = Dropout(rate=self.discriminator_dropout_rate)(x)\n",
    "            \n",
    "        # 레이어세트 수만큼 Conv + Batch + Activation + Dropout 을 깔았어\n",
    "        \n",
    "        # 마지막으로 펴주고\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # Dense Layer를 sigmoid로 통과시켜서, output 출력 output shape 어케 되냐\n",
    "        discriminator_output = Dense(1, activation ='sigmoid', kernel_initializer = self.weight_init)(x)\n",
    "        \n",
    "        discriminator = Model(discriminator_input, discriminator_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def _build_generator(self):\n",
    "            generator =Input(shape=(self.z_dim,), name = 'generator_input')\n",
    "            x = generator_input\n",
    "            \n",
    "            x = Dense(np.prod(self.generator_initial_dense_layer_size))(x)\n",
    "        # np.prod --> 각 항들의 값을 전부 곱한거 그니까 여기서 7*7*64로 generator initial dense layer를 잡았는데... 일단 잡았어\n",
    "        \n",
    "        \n",
    "        if self.generator_batch_norm_momentum :\n",
    "            x= BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "        x = Activation(self.generator_activation)(x)\n",
    "        \n",
    "        # 맨처음에 배치정규화랑 Relu를 통과시키고 그걸 7 7 64 텐서로 바꿔\n",
    "        \n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "        \n",
    "        if self.generator_dropout_rate :\n",
    "            x = Dropout(rate =self.generator_dropout_rate)(x)\n",
    "            \n",
    "        for i in range(self.n_layers_generator):\n",
    "            \n",
    "            x = UpSampling2D()(x)\n",
    "            x = Conv2D(\n",
    "            filters = self.generator_conv_filters[i],\n",
    "            kernel_size= self.generator_conv_kernel_size[i],\n",
    "            padding = 'same',\n",
    "            name = 'generator_conv_'+str(i))(x)\n",
    "            \n",
    "            if i < n_layers_generator -1 :\n",
    "                if self.generator_batch_norm_momentum:\n",
    "                    x = BatchNormalization(momentum =self.generator_batch_norm_momentum)(x)\n",
    "                x = Activation('relu')(x)\n",
    "            else : Activation('tanh')(x)\n",
    "            \n",
    "    \n",
    "    # 4개의 Conv2D 층 - 처음 두 개는 Upsampling 뒤에 놓입니다. 는 깃헙이고 책껄로 따라하니까 걍 Upsamling 4개 해보리네용\n",
    "    # 섞어서 해도 되나 봅니당\n",
    "    \n",
    "        generator_output =x\n",
    "        generator = Model(generator_input, generator_output)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def get_opti(self, lr):\n",
    "        if self.optimiser == 'adam':\n",
    "            opti = Adam(lr=lr, beta_1=0.5)\n",
    "        elif self.optimiser == 'rmsprop':\n",
    "            opti = RMSprop(lr=lr)\n",
    "        else:\n",
    "            opti = Adam(lr=lr)\n",
    "\n",
    "        return opti\n",
    "    \n",
    "    \n",
    "    def _build_adversarial(self):\n",
    "        \n",
    "        ### COMPILE DISCRIMINATOR\n",
    "\n",
    "        self.discriminator.compile(\n",
    "        optimizer=self.get_opti(self.discriminator_learning_rate)  \n",
    "        , loss = 'binary_crossentropy'\n",
    "        ,  metrics = ['accuracy']\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.set_trainable(self.discriminator, False)\n",
    "\n",
    "        model_input = Input(shape=(self.z_dim,), name='model_input')\n",
    "        model_output = self.discriminator(self.generator(model_input))\n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "        self.model.compile(optimizer=self.get_opti(self.generator_learning_rate) , loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        self.set_trainable(self.discriminator, True)\n",
    "\n",
    "        \n",
    "        ### 늘 여기까지는 잘하지 모델 짜는 건 쉬운 편이야 하지만 나는 compile 이랑 학습을 못하지 후후\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN Batch Normalization\n",
    "#### 1. mean = 0 stev = 1 인 NP로 정규화\n",
    "#### 2. scaling 과 shifting\n",
    "그냥 저 NP로 만들어주면 sigmoid를 지날 때 값이 0인애가 너무 많기 때문에 sigmoid를 지나면 사실 상 해당 구간에서 선형의 모습을 보인다. 분명 비선형 데이터를 넣은건데도 --> 너무 0 근처에 있지 말라고 가중치 r (scaling) b (shifting)을 더해 값을 변형해준다.\n",
    "\n",
    "#### 3. 장점\n",
    "1. tanh나 sigmoid 같은 활성화 함수에 대해 그래디언트 소실(vanishing gradient)문제가 감소한다.\n",
    "\n",
    "2. 가중치 초기화에 덜 민감하다. 가중치 초기값에 크게 의존하지 않기 때문에 05-1. 심층 신경망 학습에서 알아본 가중치 초기화 기법에 대해 크게 신경 쓰지 않아도 된다.\n",
    "\n",
    "3. 학습률(learning rate)를 크게 잡아도 gradient descent가 잘 수렴한다.\n",
    "\n",
    "4. 오버피팅을 억제한다. BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소한다.  하지만, BN로 인한 규제는 효과가 크지 않기 때문에 드롭아웃을 함께 사용하는 것이 좋다.\n",
    "\n",
    "\n",
    "#### 4. Momentum\n",
    "우리가 학습을 할 때 Train 할 때는 Batch 들의 평균과 분산을 구할 수가 있는데, 새로운 데이터가 들어왔어. 얘를 어떻게 정규화를 시킬꺼야?? 평균과 분산이 있어야할 수 있느데 없잖아. 그래서 미리 학습 시에 이동평균(Moving Average)를 저장해둬\n",
    "이 이동평균을 계산하기 위해 다시, 이동평균을 계산하기위해 --> 새로운 이동평균이 주는 영향을 제어하기 위해 momentum을 사용하고, 보통 momentum은 0.99 --> v = v * momentum + v_new * (1 - momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpSampling Layer\n",
    "VAE 에서는 Conv2DTranpose를 써서, Decode해줬는데, - 층마다 텐서의 너비와 높이를 두 배로 늘렸는데, [upsampling은 합성곱연산을 수행하기 전에 중간에 0을 채워] 왜죠 ? 뭐 둘 다 좋은데 뭐쓰는지는 니가 알아서해 Conv2DTranspose랑 Upsamling 둘 다 써"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
